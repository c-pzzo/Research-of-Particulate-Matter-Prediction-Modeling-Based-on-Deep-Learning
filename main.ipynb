{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Q9L_pXDjNznHwrpi-3xnppyk67Q8_ioU",
      "authorship_tag": "ABX9TyOXNLsXiPXtaZa2oXPVx5Oy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/papari1123/Research-of-Particulate-Matter-Prediction-Modeling-Based-on-Deep-Learning/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vudsmu6xzbVN",
        "outputId": "eb004eaa-eb8f-4fb9-ec5d-b9c281d3a1f3"
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "Created on Mon Sep 10 18:18:12 2018\n",
        "\n",
        "@author: LEE SEONGGU\n",
        "\"\"\"\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from pandas import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math \n",
        "\n",
        "base_path = '/content/drive/MyDrive/Z colab/Data_processing_0316'\n",
        "\n",
        "def grid(v1, v2) :   # Latitude, Longitude\n",
        "    RE = 6371.00877 # 지구 반경(km) \n",
        "    GRID = 5.0 # 격자 간격(km) \n",
        "    SLAT1 = 30.0 # 투영 위도1(degree) \n",
        "    SLAT2 = 60.0 # 투영 위도2(degree) \n",
        "    OLON = 126.0 # 기준점 경도(degree) \n",
        "    OLAT = 38.0 # 기준점 위도(degree) \n",
        "    XO = 43 # 기준점 X좌표(GRID) \n",
        "    YO = 136 # 기1준점 Y좌표(GRID) \n",
        "    DEGRAD = math.pi / 180.0 \n",
        "    RADDEG = 180.0 / math.pi \n",
        "    re = RE / GRID; \n",
        "    slat1 = SLAT1 * DEGRAD \n",
        "    slat2 = SLAT2 * DEGRAD \n",
        "    olon = OLON * DEGRAD \n",
        "    olat = OLAT * DEGRAD \n",
        "    sn = math.tan(math.pi * 0.25 + slat2 * 0.5) / math.tan(math.pi * 0.25 + slat1 * 0.5) \n",
        "    sn = math.log(math.cos(slat1) / math.cos(slat2)) / math.log(sn) \n",
        "    sf = math.tan(math.pi * 0.25 + slat1 * 0.5) \n",
        "    sf = math.pow(sf, sn) * math.cos(slat1) / sn \n",
        "    ro = math.tan(math.pi * 0.25 + olat * 0.5) \n",
        "    ro = re * sf / math.pow(ro, sn); \n",
        "    rs = {}; ra = math.tan(math.pi * 0.25 + (v1) * DEGRAD * 0.5) \n",
        "    ra = re * sf / math.pow(ra, sn) \n",
        "    theta = v2 * DEGRAD - olon \n",
        "    if theta > math.pi : \n",
        "            theta -= 2.0 * math.pi \n",
        "    if theta < -math.pi : \n",
        "            theta += 2.0 * math.pi \n",
        "    theta *= sn \n",
        "    rs['x'] = ra * math.sin(theta) + XO + 0.5\n",
        "    rs['y'] = ro - ra * math.cos(theta) + YO + 0.5\n",
        "    return rs['x'] , rs['y']\n",
        "\n",
        "\"\"\"\n",
        "1. cell당 하나의 station만 남도록 추리고, 추린 station은 mapping함.\n",
        "\"\"\"\n",
        "MAPPING = False\n",
        "if(MAPPING) :\n",
        "    dataset = read_csv(base_path  +'/data/미세먼지 측정소 주소_제주도.csv', header=0, index_col = 0, engine='python') \n",
        "#    Longitude =  dataset['Longitude'].values\n",
        "#    Latitude =  dataset['Latitude'].values\n",
        "#    print(len(Latitude))\n",
        "#    x = np.zeros(len(Latitude))\n",
        "#    y = np.zeros(len(Latitude))\n",
        "#    for i in range(len(Latitude)) :\n",
        "#        x[i], y[i] =  grid(Latitude[i], Longitude[i])\n",
        "#        \n",
        "    x_min =  int(dataset['X'].min())  #그리드의 최대최소값을 구함.\n",
        "    x_max =  int(dataset['X'].max())\n",
        "    y_min =  int(dataset['Y'].min())\n",
        "    y_max =  int(dataset['Y'].max())\n",
        "    \n",
        "    stations_map = np.zeros((x_max-x_min+1, y_max-y_min+1))  # station index가 행렬에 맵핑된다\n",
        "    using_stations = np.zeros(206) # 사용한 스테이션의 개수를 저장.\n",
        "    use_station_num = 0\n",
        "    for staion_index in range (dataset.shape[0]) :   # 그리드 매칭\n",
        "                x_in = dataset.iloc[staion_index,3]-x_min\n",
        "                y_in = dataset.iloc[staion_index,4]-y_min\n",
        "                if stations_map[x_in][y_in] == 0 :   \n",
        "                    stations_map[x_in][y_in] = dataset.iloc[staion_index,5]\n",
        "                    using_stations[use_station_num]= staion_index\n",
        "                    use_station_num += 1\n",
        "\n",
        "    # drop하기 (particulate stations used로 중복제거하여 변환)\n",
        "                    \n",
        "\"\"\"\n",
        "2. 측정소 코드 따오기\n",
        "\"\"\"           \n",
        "\n",
        "error = []\n",
        "\n",
        "CODE_EXTRACTION = True\n",
        "if(CODE_EXTRACTION) :\n",
        "    particledataset = read_csv(base_path  +'/data/2014년 1분기.csv', header=0, index_col = 10, engine='python')  # 측정소 코드의 소스\n",
        "    stationinfodataset = read_csv(base_path  +'/data/미세먼지 측정소 주소_final.csv', header=0, index_col = 0, engine='python') \n",
        "    station_id = list() \n",
        "    for staion_index in range (stationinfodataset.shape[0]) : \n",
        "        try:\n",
        "            name = stationinfodataset.iloc[staion_index,0]                 \n",
        "            station_id.append(particledataset.loc[name,\"측정소코드\"].head(1)[0])\n",
        "            error.append(particledataset.loc[name,\"측정소코드\"].head(1)[0])         \n",
        "        except : error.append(name)\n",
        "else :  \n",
        "    station_id = dataset['측정소코드'].values\n",
        "\"\"\"\n",
        "3. 측정소 코드에 맞추어 데이터 변환\n",
        "\"\"\"    \n",
        "DATA_TRANS1 = True   \n",
        "feature = ['측정소코드','측정일시','PM10']\n",
        "\n",
        "if(DATA_TRANS1) :   \n",
        "    file_num = 0\n",
        "    for y in range(2014,2019):\n",
        "        for qu in [1,2,3,4]:\n",
        "            print(qu)\n",
        "            file_num = file_num +1\n",
        "            if(file_num==1):\n",
        "                dataset =read_csv(base_path  +'/data/%d년 %d분기.csv' %(y,qu), header=0, index_col = 0, usecols = feature,  engine='python') \n",
        "                dataset = dataset.loc[station_id,:]\n",
        "            else:\n",
        "                tempDataset = read_csv(base_path  +'/data/%d년 %d분기.csv' %(y,qu), header=0, index_col = 0, usecols = feature, engine='python')\n",
        "                tempDataset = tempDataset.loc[station_id,:]\n",
        "                dataset = concat([dataset,tempDataset])  \n",
        "    dataset.index.names = ['station_id']\n",
        "    dataset = dataset.rename(columns={'측정일시':'date'})            \n",
        "    dataset.to_csv(base_path  +'/output/2014-2018 all_PM10_제주도.csv',header=True, index=True)\n",
        "#output은 측정소코드별 feature값을 열로 가지는 행렬\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "4. 먼지측정소 데이터를 날짜에 맞춰 병렬화\n",
        "\"\"\"\n",
        "    \n",
        "extractFeature = ['PM10']\n",
        "\n",
        "DATA_TRANS2 = True   \n",
        "\n",
        "if(DATA_TRANS2) :\n",
        "    dataset = read_csv(base_path  +'/output/2014-2018 all_PM10_제주도.csv', header=0, index_col = 1, engine='python') \n",
        "    for fea in range(len(extractFeature)): #feature별 생성\n",
        "        for i in range(len(station_id)):\n",
        "                print(i)\n",
        "                if(i==0):\n",
        "                    pmSet = dataset.loc[dataset['station_id']==station_id[i],extractFeature[fea]]\n",
        "                else:\n",
        "                        pmSet = concat([pmSet, dataset.loc[dataset['station_id']==station_id[i], extractFeature[fea]]], axis = 1)\n",
        "        pmSet.columns = station_id\n",
        "        pmSet = pmSet.drop(pmSet.index[len(pmSet.index)-1],0) # weather와 행 개수 맞추기 위해 마지막 행 제거\n",
        "        pmSet.to_csv(base_path  +'/output/2014-2018 all_제주도'+extractFeature[fea]+'.csv',header=True, index=True)\n",
        "           \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_handle_usecols\u001b[0;34m(self, columns, usecols_key)\u001b[0m\n\u001b[1;32m   2838\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2839\u001b[0;31m                             \u001b[0mcol_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musecols_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2840\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: '측정소코드' is not in list",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9140a645a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mfile_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_num\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_num\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m  \u001b[0;34m+\u001b[0m\u001b[0;34m'/data/%d년 %d분기.csv'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstation_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0;34m'are \"c\", \"python\", or \"python-fwf\")'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 )\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_original_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2407\u001b[0;31m             ) = self._infer_columns()\n\u001b[0m\u001b[1;32m   2408\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2781\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2783\u001b[0;31m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_usecols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_handle_usecols\u001b[0;34m(self, columns, usecols_key)\u001b[0m\n\u001b[1;32m   2839\u001b[0m                             \u001b[0mcol_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musecols_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2840\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2841\u001b[0;31m                             \u001b[0m_validate_usecols_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2842\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m                         \u001b[0mcol_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[0;34m(usecols, names)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1305\u001b[0;31m             \u001b[0;34mf\"Usecols do not match columns, columns expected but not found: {missing}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m         )\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['측정소코드', '측정일시']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUkNC-20t-SE"
      },
      "source": [
        ""
      ]
    }
  ]
}